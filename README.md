https://motion-caption.github.io/
# Motion-caption: Datasets with more detailed action descriptions

---

## ðŸ‘€ Introduce to Motion-caption

We first preprocess existing datasets to ensure that the captions contain visual information and include more detailed
action descriptions. Additionally, we integrate multi-source data, combining human-labeled action data with virtual 
action data generated using Unity3D. Through effective data fusion and a staged training strategy, we fine-tune existing
large-scale video understanding models to generate captions with richer dynamic information. Experimental results 
demonstrate significant performance improvements across multiple datasets, especially in capturing and generating 
action-related descriptions, with notable advancements compared to the original models. The proposed method offers a new
approach for capturing dynamic information in video understanding models and provides a more practical solution for 
real-world applications.

## Datasets

In this section, we will provide a brief introduction to the construction methods of the four sub-datasets in the Motion Caption 
Dataset

### Human-labeled Video Caption

### Video Caption Constructed via Unity3D

### Video Caption Constructed Based on 2D Videos

### Video Caption Constructed through an Automated Pipeline
